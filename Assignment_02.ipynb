{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 02.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajibmondal/CS224n/blob/master/Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYWvyLMH-cYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "# First implement a gradient checker by filling in the following functions\n",
        "def gradcheck_naive(f, x, gradientText):\n",
        "    \"\"\" Gradient check for a function f.\n",
        "    Arguments:\n",
        "    f -- a function that takes a single argument and outputs the\n",
        "         loss and its gradients\n",
        "    x -- the point (numpy array) to check the gradient at\n",
        "    gradientText -- a string detailing some context about the gradient computation\n",
        "    \"\"\"\n",
        "\n",
        "    rndstate = random.getstate()\n",
        "    random.setstate(rndstate)\n",
        "    fx, grad = f(x) # Evaluate function value at original point\n",
        "    h = 1e-4        # Do not change this!\n",
        "\n",
        "    # Iterate over all indexes ix in x to check the gradient.\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        x[ix] += h # increment by h\n",
        "        random.setstate(rndstate)\n",
        "        fxh, _ = f(x) # evalute f(x + h)\n",
        "        x[ix] -= 2 * h # restore to previous value (very important!)\n",
        "        random.setstate(rndstate)\n",
        "        fxnh, _ = f(x)\n",
        "        x[ix] += h\n",
        "        numgrad = (fxh - fxnh) / 2 / h\n",
        "\n",
        "        # Compare gradients\n",
        "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
        "        if reldiff > 1e-5:\n",
        "            print(\"Gradient check failed for %s.\" % gradientText)\n",
        "            print(\"First gradient error found at index %s in the vector of gradients\" % str(ix))\n",
        "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
        "                grad[ix], numgrad))\n",
        "            return\n",
        "\n",
        "        it.iternext() # Step to next dimension\n",
        "\n",
        "    print(\"Gradient check passed!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bkhl08p-ee4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "class StanfordSentiment:\n",
        "    def __init__(self, path=None, tablesize = 1000000):\n",
        "        if not path:\n",
        "            path = \"utils/datasets/stanfordSentimentTreebank\"\n",
        "\n",
        "        self.path = path\n",
        "        self.tablesize = tablesize\n",
        "\n",
        "    def tokens(self):\n",
        "        if hasattr(self, \"_tokens\") and self._tokens:\n",
        "            return self._tokens\n",
        "\n",
        "        tokens = dict()\n",
        "        tokenfreq = dict()\n",
        "        wordcount = 0\n",
        "        revtokens = []\n",
        "        idx = 0\n",
        "\n",
        "        for sentence in self.sentences():\n",
        "            for w in sentence:\n",
        "                wordcount += 1\n",
        "                if not w in tokens:\n",
        "                    tokens[w] = idx\n",
        "                    revtokens += [w]\n",
        "                    tokenfreq[w] = 1\n",
        "                    idx += 1\n",
        "                else:\n",
        "                    tokenfreq[w] += 1\n",
        "\n",
        "        tokens[\"UNK\"] = idx\n",
        "        revtokens += [\"UNK\"]\n",
        "        tokenfreq[\"UNK\"] = 1\n",
        "        wordcount += 1\n",
        "\n",
        "        self._tokens = tokens\n",
        "        self._tokenfreq = tokenfreq\n",
        "        self._wordcount = wordcount\n",
        "        self._revtokens = revtokens\n",
        "        return self._tokens\n",
        "\n",
        "    def sentences(self):\n",
        "        if hasattr(self, \"_sentences\") and self._sentences:\n",
        "            return self._sentences\n",
        "\n",
        "        sentences = []\n",
        "        with open(self.path + \"/datasetSentences.txt\", \"r\") as f:\n",
        "            first = True\n",
        "            for line in f:\n",
        "                if first:\n",
        "                    first = False\n",
        "                    continue\n",
        "\n",
        "                splitted = line.strip().split()[1:]\n",
        "                # Deal with some peculiar encoding issues with this file\n",
        "                sentences += [[w.lower() for w in splitted]]\n",
        "\n",
        "        self._sentences = sentences\n",
        "        self._sentlengths = np.array([len(s) for s in sentences])\n",
        "        self._cumsentlen = np.cumsum(self._sentlengths)\n",
        "\n",
        "        return self._sentences\n",
        "\n",
        "    def numSentences(self):\n",
        "        if hasattr(self, \"_numSentences\") and self._numSentences:\n",
        "            return self._numSentences\n",
        "        else:\n",
        "            self._numSentences = len(self.sentences())\n",
        "            return self._numSentences\n",
        "\n",
        "    def allSentences(self):\n",
        "        if hasattr(self, \"_allsentences\") and self._allsentences:\n",
        "            return self._allsentences\n",
        "\n",
        "        sentences = self.sentences()\n",
        "        rejectProb = self.rejectProb()\n",
        "        tokens = self.tokens()\n",
        "        allsentences = [[w for w in s\n",
        "            if 0 >= rejectProb[tokens[w]] or random.random() >= rejectProb[tokens[w]]]\n",
        "            for s in sentences * 30]\n",
        "\n",
        "        allsentences = [s for s in allsentences if len(s) > 1]\n",
        "\n",
        "        self._allsentences = allsentences\n",
        "\n",
        "        return self._allsentences\n",
        "\n",
        "    def getRandomContext(self, C=5):\n",
        "        allsent = self.allSentences()\n",
        "        sentID = random.randint(0, len(allsent) - 1)\n",
        "        sent = allsent[sentID]\n",
        "        wordID = random.randint(0, len(sent) - 1)\n",
        "\n",
        "        context = sent[max(0, wordID - C):wordID]\n",
        "        if wordID+1 < len(sent):\n",
        "            context += sent[wordID+1:min(len(sent), wordID + C + 1)]\n",
        "\n",
        "        centerword = sent[wordID]\n",
        "        context = [w for w in context if w != centerword]\n",
        "\n",
        "        if len(context) > 0:\n",
        "            return centerword, context\n",
        "        else:\n",
        "            return self.getRandomContext(C)\n",
        "\n",
        "    def sent_labels(self):\n",
        "        if hasattr(self, \"_sent_labels\") and self._sent_labels:\n",
        "            return self._sent_labels\n",
        "\n",
        "        dictionary = dict()\n",
        "        phrases = 0\n",
        "        with open(self.path + \"/dictionary.txt\", \"r\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line: continue\n",
        "                splitted = line.split(\"|\")\n",
        "                dictionary[splitted[0].lower()] = int(splitted[1])\n",
        "                phrases += 1\n",
        "\n",
        "        labels = [0.0] * phrases\n",
        "        with open(self.path + \"/sentiment_labels.txt\", \"r\") as f:\n",
        "            first = True\n",
        "            for line in f:\n",
        "                if first:\n",
        "                    first = False\n",
        "                    continue\n",
        "\n",
        "                line = line.strip()\n",
        "                if not line: continue\n",
        "                splitted = line.split(\"|\")\n",
        "                labels[int(splitted[0])] = float(splitted[1])\n",
        "\n",
        "        sent_labels = [0.0] * self.numSentences()\n",
        "        sentences = self.sentences()\n",
        "        for i in range(self.numSentences()):\n",
        "            sentence = sentences[i]\n",
        "            full_sent = \" \".join(sentence).replace('-lrb-', '(').replace('-rrb-', ')')\n",
        "            sent_labels[i] = labels[dictionary[full_sent]]\n",
        "\n",
        "        self._sent_labels = sent_labels\n",
        "        return self._sent_labels\n",
        "\n",
        "    def dataset_split(self):\n",
        "        if hasattr(self, \"_split\") and self._split:\n",
        "            return self._split\n",
        "\n",
        "        split = [[] for i in range(3)]\n",
        "        with open(self.path + \"/datasetSplit.txt\", \"r\") as f:\n",
        "            first = True\n",
        "            for line in f:\n",
        "                if first:\n",
        "                    first = False\n",
        "                    continue\n",
        "\n",
        "                splitted = line.strip().split(\",\")\n",
        "                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n",
        "\n",
        "        self._split = split\n",
        "        return self._split\n",
        "\n",
        "    def getRandomTrainSentence(self):\n",
        "        split = self.dataset_split()\n",
        "        sentId = split[0][random.randint(0, len(split[0]) - 1)]\n",
        "        return self.sentences()[sentId], self.categorify(self.sent_labels()[sentId])\n",
        "\n",
        "    def categorify(self, label):\n",
        "        if label <= 0.2:\n",
        "            return 0\n",
        "        elif label <= 0.4:\n",
        "            return 1\n",
        "        elif label <= 0.6:\n",
        "            return 2\n",
        "        elif label <= 0.8:\n",
        "            return 3\n",
        "        else:\n",
        "            return 4\n",
        "\n",
        "    def getDevSentences(self):\n",
        "        return self.getSplitSentences(2)\n",
        "\n",
        "    def getTestSentences(self):\n",
        "        return self.getSplitSentences(1)\n",
        "\n",
        "    def getTrainSentences(self):\n",
        "        return self.getSplitSentences(0)\n",
        "\n",
        "    def getSplitSentences(self, split=0):\n",
        "        ds_split = self.dataset_split()\n",
        "        return [(self.sentences()[i], self.categorify(self.sent_labels()[i])) for i in ds_split[split]]\n",
        "\n",
        "    def sampleTable(self):\n",
        "        if hasattr(self, '_sampleTable') and self._sampleTable is not None:\n",
        "            return self._sampleTable\n",
        "\n",
        "        nTokens = len(self.tokens())\n",
        "        samplingFreq = np.zeros((nTokens,))\n",
        "        self.allSentences()\n",
        "        i = 0\n",
        "        for w in range(nTokens):\n",
        "            w = self._revtokens[i]\n",
        "            if w in self._tokenfreq:\n",
        "                freq = 1.0 * self._tokenfreq[w]\n",
        "                # Reweigh\n",
        "                freq = freq ** 0.75\n",
        "            else:\n",
        "                freq = 0.0\n",
        "            samplingFreq[i] = freq\n",
        "            i += 1\n",
        "\n",
        "        samplingFreq /= np.sum(samplingFreq)\n",
        "        samplingFreq = np.cumsum(samplingFreq) * self.tablesize\n",
        "\n",
        "        self._sampleTable = [0] * self.tablesize\n",
        "\n",
        "        j = 0\n",
        "        for i in range(self.tablesize):\n",
        "            while i > samplingFreq[j]:\n",
        "                j += 1\n",
        "            self._sampleTable[i] = j\n",
        "\n",
        "        return self._sampleTable\n",
        "\n",
        "    def rejectProb(self):\n",
        "        if hasattr(self, '_rejectProb') and self._rejectProb is not None:\n",
        "            return self._rejectProb\n",
        "\n",
        "        threshold = 1e-5 * self._wordcount\n",
        "\n",
        "        nTokens = len(self.tokens())\n",
        "        rejectProb = np.zeros((nTokens,))\n",
        "        for i in range(nTokens):\n",
        "            w = self._revtokens[i]\n",
        "            freq = 1.0 * self._tokenfreq[w]\n",
        "            # Reweigh\n",
        "            rejectProb[i] = max(0, 1 - np.sqrt(threshold / freq))\n",
        "\n",
        "        self._rejectProb = rejectProb\n",
        "        return self._rejectProb\n",
        "\n",
        "    def sampleTokenIdx(self):\n",
        "        return self.sampleTable()[random.randint(0, self.tablesize - 1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No0OCXdS-wm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def normalizeRows(x):\n",
        "    \"\"\" Row normalization function\n",
        "    Implement a function that normalizes each row of a matrix to have\n",
        "    unit length.\n",
        "    \"\"\"\n",
        "    N = x.shape[0]\n",
        "    x /= np.sqrt(np.sum(x**2, axis=1)).reshape((N,1)) + 1e-30\n",
        "    return x\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute the softmax function for each row of the input x.\n",
        "    It is crucial that this function is optimized for speed because\n",
        "    it will be used frequently in later code. \n",
        "    Arguments:\n",
        "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
        "    Return:\n",
        "    x -- You are allowed to modify x in-place\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "\n",
        "    if len(x.shape) > 1:\n",
        "        # Matrix\n",
        "        tmp = np.max(x, axis=1)\n",
        "        x -= tmp.reshape((x.shape[0], 1))\n",
        "        x = np.exp(x)\n",
        "        tmp = np.sum(x, axis=1)\n",
        "        x /= tmp.reshape((x.shape[0], 1))\n",
        "    else:\n",
        "        # Vector\n",
        "        tmp = np.max(x)\n",
        "        x -= tmp\n",
        "        x = np.exp(x)\n",
        "        tmp = np.sum(x)\n",
        "        x /= tmp\n",
        "\n",
        "    assert x.shape == orig_shape\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZVQ9f-v-wrG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "d2add527-1a87-456b-b947-71fe64fe297a"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from utils.gradcheck import gradcheck_naive\n",
        "from utils.utils import normalizeRows, softmax\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1ae3688f030a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradcheck\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalizeRows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMJE5OPV-w4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1KfmpFL-w76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbXlN3Oa-w1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B79gJVE8-w0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lKtRY1i-wyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK9lZ-qs-wwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5pQ1Lrw-wuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L61SvA_S-ekw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FclEsEcQ-eit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oMLAZmp-ecm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmKaH1jC-eZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}